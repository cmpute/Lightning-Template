# ---------- Common settings ----------
nodes: 1 # Number of nodes to be used for training
devices: 'auto' # Specify 'cpu'/'gpu', or number of devices, or sequence of device indices.
                # You can also use 'auto' to automatically choose all devices
seed: 12345 # The random seed across all libraries
experiment: "example" # Name of the experiment. It determines the name of the root result folder
version: "" # Name of the version. It determines the name of the sub folders.
            # The version name will be appended with the date and time as well.
output_path: "output" # The root folder of all outputs

log_level: info # This determines the logging level for console output. The output level for the log file will always be INFO.


# ---------- Training related settings ----------
train:

  # common configs
  batch_size: 256 # per gpu
  num_workers: 4 # per node
  epochs: 100 # total epochs


  # optimization related
  lr: 1.e-4
  lr_scheduler: # Learning rate scheduler settings
    type: reduce_on_plateau # Options: step, reduce_on_plateau, one_cycle, ..

    ##### Type specific params ####
    factor: 0.2
    patience: 20
    min_lr: 5.e-5

  optimizer: # Optimizer settings
    type: adam # Options: sgd, adam, adamw, ..

    ##### Type specific params ####
    weight_decay: 0


  # auxilary configs
  save_every_n_minutes: 10 # If nonzero, then the checkpoints will be saved every this minutes as well
  fast_mul: false # Whether enable fast 32-bit multiplications
  visualize_interval: 10 # Generate visualization every this number of epochs


  ##### Params that will be directly passed to the Lightning Trainer #####
  # see https://lightning.ai/docs/pytorch/latest/common/trainer.html#trainer-flags for all available options
  other:
    precision: 32 # Available options: 16, bf16, 16-mixed, bf16-mixed, 32, 64
    sync_batchnorm: true # Whether to use the SyncBatchNorm

    limit_train_batches: 1.0 # Use only this fraction of the dataset as 1 epoch
                             # Note that if you want to use full dataset, this value can be null or 1.0, but not 1!

    enable_model_summary: false

# ---------- Testing related settings ----------
test:

  # common configs
  batch_size: 512 # per gpu
  num_workers: 8 # per node
